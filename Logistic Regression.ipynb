{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9769e7b3-0af4-4034-8501-27c458d80058",
   "metadata": {},
   "source": [
    "# Logistic Regression with a Neural Network \r",
    "implemented as a neural network without hidden layers, where the output layer consists of a single unit followed by a sigmoid activation function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4f5dab7-a0a4-414e-9830-3951e1cd3d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ed161a-c6f9-47a1-86b0-4beeaf9a4b46",
   "metadata": {},
   "source": [
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$\n",
    "\n",
    "$z^{(i)} = w^T x^{(i)} + b $\n",
    "\n",
    "$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})$\n",
    "\n",
    "$\\mathcal{L}(a^{(i)}, Y^{(i)})  = - y^{(i)} \\log(a^{(i)} - (1-y^{(i)} )  \\log(1-a^{(i)})$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "\n",
    "$J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4422643c-a274-4886-ad94-791118c62e4d",
   "metadata": {},
   "source": [
    "**sigmoid function** \n",
    "\n",
    " $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w^T x + b$ to make predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7aa0cbe8-8c3d-4797-ba6e-1137f113809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    s = 1 / (1 + np.exp(-z))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ed16ab-a054-4743-9648-4e1babc6d9ce",
   "metadata": {},
   "source": [
    "### Forward and Backward propagation\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "Forward Propagation:\n",
    "- We have X\n",
    "- We compute $A = \\sigma(w^T X + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- We calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}(y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)}))$\n",
    "\n",
    "Here are the two formulas we will be using: \n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b345f405-6414-43e1-be7e-e9898e2e9da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    L = Y * np.log(A) + (1 - Y) * np.log(1 - A)\n",
    "    cost = (-1 / m) * (np.sum(L, axis = 1, keepdims=True))\n",
    "    \n",
    "    dw = (1/m) * (np.dot(X, (A - Y).T))\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "    \n",
    "    cost = np.squeeze(np.array(cost))\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c50204-c77b-4923-8c79-a41938940a31",
   "metadata": {},
   "source": [
    "**Optimization**\r\n",
    "\r\n",
    "We want to update the parameters using gradient descent.The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate.ate.\r\n",
    "ate.\r\n",
    "ng rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a7aa35c-5b51-4c1b-9f5c-68c48c38c292",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations=100, learning_rate=0.009, print_cost=False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    w = copy.deepcopy(w)\n",
    "    b = copy.deepcopy(b)\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "    \n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "    \n",
    "        w = w - learning_rate * dw\n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "            # Print the cost every 100 training iterations\n",
    "            if print_cost:\n",
    "                print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "    \n",
    "    params = {\"w\": w,\n",
    "              \"b\": b}\n",
    "    \n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "    \n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ae59fd-21a0-4f20-a8ec-370cd4b9915f",
   "metadata": {},
   "source": [
    "**predict**\n",
    "\n",
    "1 - Calculate $\\hat{Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "2 - Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector `Y_prediction`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1118a3f3-9cef-4990-b3a4-4cf3e922983c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    Y_prediction = np.zeros((1, m))\n",
    "    w = w.reshape(X.shape[0], 1)\n",
    "    \n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    \n",
    "    Y_prediction = np.where(A > 0.5, 1, 0)\n",
    "    \n",
    "    return Y_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf84e4b-d7c7-42fa-939f-536a28582868",
   "metadata": {},
   "source": [
    "### Merge all functions into a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ddd8881-6c79-4a1e-a4c2-e3ba51ec88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "    \"\"\"\n",
    "    Builds the logistic regression model by calling the function you've implemented previously\n",
    "    \n",
    "    Returns:\n",
    "    d -- dictionary containing information about the model.\n",
    "    \"\"\"\n",
    "    w, b = np.zeros((X_train.shape[0], 1)), 0.0\n",
    "    \n",
    "    params, grads, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost=False)\n",
    "    \n",
    "    w = params[\"w\"]\n",
    "    b = params[\"b\"]\n",
    "    Y_prediction_test = predict(w, b, X_test)\n",
    "    Y_prediction_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    if print_cost:\n",
    "        print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n",
    "        print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(Y_prediction_test - Y_test)) * 100))\n",
    "\n",
    "    \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": Y_prediction_test, \n",
    "         \"Y_prediction_train\" : Y_prediction_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
