{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6eea1c07-b1a4-4921-b7f0-1044741199f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f4bb8d-ea9c-4358-9f37-8af15e9689ba",
   "metadata": {},
   "source": [
    "### Forward propagation Model \n",
    "\n",
    "**Forward propagation for single layer:**\n",
    "\n",
    "$Z^{[l]} = W^{[l]}.A^{[l-1]} + b^{[l]}$\n",
    "\n",
    "$A^{[l]} = g(Z^{[l]})$\n",
    "\n",
    "we should cashe $A^{[l-1]}$, $W^{[l]}$, $b^{[l]}$, $Z^{[l]}$ for using them in backward propagation, so we have:\n",
    "\n",
    "linear_cashe = ($A^{[l-1]}$, $W^{[l]}$, $b^{[l]}$)\n",
    "\n",
    "cashe = (linear_cashe, $Z^{[l]}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4142424e-f688-48e2-8637-9a8e12d29ce4",
   "metadata": {},
   "source": [
    "**relu activation function**\n",
    "\n",
    "$g(Z) = max(0, Z)$ for $z = w.a + b$ \n",
    "\n",
    "**sigmoid function** \n",
    "\n",
    " $g(Z) = sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w.a + b$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5c4018c-f231-4ed2-bcb1-00cc9267b87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb3c8299-96b2-4cfe-ae56-c19ea6b9a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(A_prev, W, b, activation):\n",
    "    Z = np.dot(W, A_prev) + b\n",
    "    linear_cashe = (A_prev, W, b)\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        A = relu(Z)\n",
    "    elif activation == \"sigmoid\": \n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    cashe = (linear_cashe, Z)\n",
    "\n",
    "    return A, cashe\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16493ad6-b63a-43a2-aa28-33ca973ab756",
   "metadata": {},
   "source": [
    "### Implementation of initialization for an L-layer Neural Network. \r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d57512f-bf9b-4676-bbb0-2da65231b841",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims) # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        \n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "               \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l - 1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8429371f-6344-49c2-a94f-d23b1f5fdabd",
   "metadata": {},
   "source": [
    "### L-Layer Model\n",
    "\n",
    "Implementation of the forward propagation for L-Layer Model.\r\n",
    "\r\n",
    "**Instructions**: In the code below, the variable `AL` will denote $A^{[L]} = \\sigma(Z^{[L]}) = \\sigma(W^{[L]} A^{[L-1]} + b^{[L]})$. (This is sometimes also called `Yhat`, i.e., this is $\\hat{Y}$.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "630fd455-bde9-49e4-9511-e1d0d393ae43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = forward_prop(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = forward_prop(A, parameters['W' + str(L)], parameters['b' + str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db036a32-0ad7-4da5-b74a-43e496cb770b",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "Compute the cross-entropy cost $J$, using the following formula:\n",
    "\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L] (i)}\\right) \\large{)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8533884-79c1-4ea5-9dd3-2cdd4477e740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation abov\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    logprobs = np.multiply(np.log(AL),Y) + np.multiply(np.log(1 - AL),1 - Y)\n",
    "    cost = - (1/m) * np.sum(logprobs)\n",
    "    \n",
    "    cost = float(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f693625e-6cdb-413a-9010-d94f70307160",
   "metadata": {},
   "source": [
    "### Backward Propagation Module\n",
    "\n",
    "**Backward Propagation for single layer:**\n",
    "\n",
    "$dZ^{[l]} = dA^{[l]}*g^{[l]'}(Z^{[l]})$\n",
    "\n",
    "$dW^{[l]} = \\frac{1}{m}dZ^{[l]} A^{[l-1]T}$\n",
    "\n",
    "$db^{[l]} = \\frac{1}{m} np.sum(dZ^{[l]}, axis = 1, keepdims=True)$\n",
    "\n",
    "$dA^{[l-1]} = W^{[l]T}dZ^{[l]}$\n",
    "\n",
    "to implement $g^{[1]'}$ for relu function:\n",
    "\n",
    "$$g^{[1]'} = \\begin{cases}\n",
    "      1 & \\text{if}\\ Z > 0 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$$\n",
    "\n",
    "to implement $g^{[1]'}$ for sigmoid function:\n",
    "\n",
    "$$s = sigmoid(Z)$$\n",
    "\n",
    "$$g^{[1]'} = s * (1-s)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "968db66b-b5c9-4af9-ae92-63f57a24b69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, Z):\n",
    "    \n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "def sigmoid_backward(dA, Z):\n",
    "    \n",
    "    s = sigmoid(Z)\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4f2a839a-024c-4863-b791-168fcdcc6a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(dA, cache, activation):\n",
    "\n",
    "    linear_cache, Z = cache\n",
    "    A_prev, W, b = linear_cache\n",
    "\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, Z)\n",
    "\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, Z)\n",
    "\n",
    "    dW = (1 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return dA_prev, dW, db\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10c24f39-8d00-4730-985f-684beef91c4f",
   "metadata": {},
   "source": [
    "###  L-Model Backward \n",
    "\n",
    "**Initializing backpropagation**:\n",
    "\n",
    "\n",
    "To backpropagate through this network, we know that the output is:\n",
    "\n",
    "$A^{[L]} = \\sigma(Z^{[L]})$\n",
    "\n",
    "The code thus needs to compute `dAL` $= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$\n",
    "\n",
    "To do so, we use this formula:\n",
    "```python\r\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) # derivative of cost with respect to AL\r\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f3f411f-b86a-4b30-a9bb-e0f48d4fdcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "\n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    #Lth layer (SIGMOID)\n",
    "    dA_prev, dW, db = backward_prop(dAL, caches[L-1], \"sigmoid\")\n",
    "    grads[\"dA\" + str(L-1)] = dA_prev\n",
    "    grads[\"dW\" + str(L)] = dW\n",
    "    grads[\"db\" + str(L)] = db\n",
    "\n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU).\n",
    "        dA_prev, dW, db = backward_prop(grads[\"dA\" + str(l+1)], caches[l], \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "        grads[\"dW\" + str(l + 1)] =dW\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd34ed9-a30c-4895-a0ac-cd72a48be8a0",
   "metadata": {},
   "source": [
    "### Update Parameters Using Gradient Descent\n",
    "\n",
    "**General gradient descent rule**: $\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1712b7f7-2a2c-4195-bafa-2120d2c45ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(params, grads, learning_rate):\n",
    "   \n",
    "    parameters = copy.deepcopy(params)\n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a3326d92-42cf-44f2-afe6-c968a2b43c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_layer_model(X, Y, dim_layers, num_iterations = 1000, learning_rate = 0.01, print_cost = False):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    costs = []\n",
    "    parameters = initialize_parameters_deep(dim_layers)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        \n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        accuracy = np.mean((AL >= 0.5) == Y) * 100  # Calculate accuracy\n",
    "        cost = compute_cost(AL, Y)\n",
    "    \n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0 or i == num_iterations - 1:\n",
    "            print(\"Cost after iteration %i: %f, Accuracy: %.2f%%\" % (i, cost, accuracy))\n",
    "        if i % 1000 == 0 or i == num_iterations:\n",
    "            costs.append(cost)\n",
    "\n",
    "    return parameters, costs\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9916e7ff-af78-4918-b742-8983409b26e8",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "$y_{prediction} = \\mathbb 1 * \\text{(activation > 0.5)} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9d2ba7c3-d310-4bd5-9e54-4fe889bcee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, parameters):\n",
    "    AL, caches = L_model_forward(X, parameters)\n",
    "    AL = np.where(AL > 0.5, 1, 0)\n",
    "    return AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "0d3fc695-e984-4a27-a87e-3e93970b6367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693310, Accuracy: 57.14%\n",
      "Cost after iteration 1000: 0.472283, Accuracy: 85.71%\n",
      "Cost after iteration 2000: 0.262052, Accuracy: 85.71%\n",
      "Cost after iteration 3000: 0.175653, Accuracy: 100.00%\n",
      "Cost after iteration 4000: 0.127086, Accuracy: 100.00%\n",
      "Cost after iteration 5000: 0.099495, Accuracy: 100.00%\n",
      "Cost after iteration 6000: 0.080136, Accuracy: 100.00%\n",
      "Cost after iteration 7000: 0.065600, Accuracy: 100.00%\n",
      "Cost after iteration 8000: 0.054514, Accuracy: 100.00%\n",
      "Cost after iteration 9000: 0.045941, Accuracy: 100.00%\n",
      "Cost after iteration 9999: 0.039232, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Updated sample data\n",
    "X = np.array([[0, 1, 4, 5, 2, 5, 3], [2, 3, 5, 6, 2, 7, 4]]) # Input features ,shape=(nx, m), m:training examples, nx: #features\n",
    "Y = np.array([[0, 0, 1, 1, 0, 1, 0]])                   # Output labels\n",
    "\n",
    "parameters, costs = L_layer_model(X, Y, [2, 5, 1], num_iterations = 10000, learning_rate = 0.01, print_cost = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a6adf4e1-4e55-464a-8d80-04c1f89da5cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01781109,  0.00429925],\n",
       "       [ 0.00096497, -0.01863493],\n",
       "       [-0.00277388, -0.00354759],\n",
       "       [-0.00082741, -0.00627001],\n",
       "       [-0.00043818, -0.00477218]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parameters[\"W1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "05a08ba1-92bd-41c0-8e20-eac960f14b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "[[0 0 1 1 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(X, parameters)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2fb08-e49e-46a3-a342-225d9055fa50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
