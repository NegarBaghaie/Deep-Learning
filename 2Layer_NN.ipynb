{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8eea09a6-8608-438a-80c3-23d9fae565b0",
   "metadata": {},
   "source": [
    "## 2 Layer Neural Network model\n",
    "a Neural Network with a single hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2025857b-c73a-4ba0-ae4a-aaa21ac4df18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f960eb0-f925-48df-afba-cf33cd1eabf1",
   "metadata": {},
   "source": [
    "**relu activation function**\n",
    "\n",
    "$g(Z) = max(0, Z)$ for $z = w.x + b$ \n",
    "\n",
    "**sigmoid function** \n",
    "\n",
    " $sigmoid(z) = \\frac{1}{1 + e^{-z}}$ for $z = w.x + b$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a94598c0-0dfc-40f7-9aba-504b3670ed09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d369ed73-20ce-40df-bcc5-1558dd4014ae",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "We Implement `forward_prop()` using the following equations:\n",
    "\n",
    "$Z^{[1]} =  W^{[1]} X + b^{[1]}$\n",
    "\n",
    "$A^{[1]} = relu(Z^{[1]})$\n",
    "\n",
    "$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$\n",
    "\n",
    "$\\hat{Y} = A^{[2]} = \\sigma(Z^{[2]})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "021c8cd9-cead-41eb-9efd-d618ae82fc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(W1, b1, W2, b2, X):\n",
    "    Z1 = np.dot(W1, X) + b1\n",
    "    A1 = relu(Z1)\n",
    "    Z2 = np.dot(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    return Z1, A1, Z2, A2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c698883e-6de3-406b-9832-7c94e5fd17e7",
   "metadata": {},
   "source": [
    "### Cost Function\n",
    "After $A^{[2]}$ (in the Python variable \"`A2`\"), which contains $a^{[2](i)}$ for all examples, the cost function is as follows:\n",
    "$$J = - \\frac{1}{m} \\sum\\limits_{i = 1}^{m} \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6adf884-575e-4457-b064-0be4eb0a6f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation abov\n",
    "    \n",
    "    \"\"\"\n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "    logprobs = np.multiply(np.log(A2),Y) + np.multiply(np.log(1 - A2),1 - Y)\n",
    "    cost = - (1/m) * np.sum(logprobs)\n",
    "    \n",
    "    cost = float(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dab2688-aff3-4a71-8950-7323b730902a",
   "metadata": {},
   "source": [
    "### Backward propagation\n",
    "\n",
    "We Implement `back_prop()` using the following equations:\n",
    "\n",
    "$dZ^{[2]} = A^{[2]} - Y$ \n",
    "\n",
    "$dW^{[2]} = \\frac{1}{m}dZ^{[2]} A^{[1]T}$\n",
    "\n",
    "$db^{[2]} = \\frac{1}{m} np.sum(dZ^{[2]}, axis = 1, keepdims=True)$\n",
    "\n",
    "$dZ^{[1]} = W^{[1]T}dZ^{[2]}*g^{[1]'}(Z^{[1]})$\n",
    "\n",
    "$dW^{[1]} = \\frac{1}{m}dZ^{[1]} X^{T}$\n",
    "\n",
    "$db^{[2]} = \\frac{1}{m} np.sum(dZ^{[1]}, axis = 1, keepdims=True)$\n",
    "\n",
    "to implement $g^{[1]'}$ for relu function:\n",
    "\n",
    "$$g^{[1]'} = \\begin{cases}\n",
    "      1 & \\text{if}\\ Z > 0 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ee569bf-b8c0-4d8e-9e65-39d338e18ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drelu(Z):\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "    \n",
    "def back_prop(W1, b1, W2, b2, X, Y, Z1, A1, Z2, A2):\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    \n",
    "    dZ2 = A2 - Y\n",
    "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
    "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T, dZ2) * drelu(Z1)\n",
    "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
    "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "    \n",
    "    return dW2, db2, dW1, db1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4665e43a-4490-45a2-986b-b3da514945b5",
   "metadata": {},
   "source": [
    "### Update Parameters Using Gradient Descent\n",
    "\n",
    "**General gradient descent rule**: $\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$ where $\\alpha$ is the learning rate and $\\theta$ represents a parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f1605fc-cf9b-4112-8793-921b212fe575",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(W1, b1, W2, b2, X, Y, iterations, alpha, print_cost=False):\n",
    "    for i in range(iterations):\n",
    "        \n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        accuracy = np.mean((A2 >= 0.5) == Y) * 100  # Calculate accuracy\n",
    "        \n",
    "        dW2, db2, dW1, db1 = back_prop(W1, b1, W2, b2, X, Y, Z1, A1, Z2, A2)\n",
    "        \n",
    "        W1 = W1 - alpha * dW1\n",
    "        b1 = b1 - alpha * db1\n",
    "        W2 = W2 - alpha * dW2\n",
    "        b2 = b2 - alpha * db2\n",
    "        \n",
    "        # Print the cost and accuracy every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration %i: %f, Accuracy: %.2f%%\" % (i, cost, accuracy))\n",
    "            \n",
    "    return W1, b1, W2, b2, A2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7d49f506-6c2e-4ec1-8f33-3d8797421e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False, learning_rate = 0.01 ):\n",
    "    \n",
    "    # Define the dimensions of the neural network\n",
    "    input_size = X.shape[0]\n",
    "    hidden_size = n_h\n",
    "    output_size = Y.shape[0]\n",
    "    \n",
    "    # Initialize weights and biases\n",
    "    W1 = np.random.randn(hidden_size, input_size) * 0.01\n",
    "    b1 = np.zeros((hidden_size, 1))\n",
    "    W2 = np.random.randn(output_size, hidden_size) * 0.01\n",
    "    b2 = np.zeros((output_size, 1))\n",
    "\n",
    "    # Train the neural network\n",
    "    W1, b1, W2, b2, A2 = gradient_descent(W1, b1, W2, b2, X, Y, num_iterations, learning_rate, print_cost)\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "\n",
    "    return parameters\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed639b0f-20c1-44b1-b52b-90e26fdee9de",
   "metadata": {},
   "source": [
    "### Predict\n",
    "\n",
    "$y_{prediction} = \\mathbb 1 * \\text{(activation > 0.5)} = \\begin{cases}\n",
    "      1 & \\text{if}\\ activation > 0.5 \\\\\n",
    "      0 & \\text{otherwise}\n",
    "    \\end{cases}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "17787820-1fa1-431f-933b-e2ed2cd1b45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(W1, b1, W2, b2, X):\n",
    "    \n",
    "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "    predictions = np.round(A2)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "09b335ca-8e05-4240-9057-fc60ff4e641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693179, Accuracy: 57.14%\n",
      "Cost after iteration 1000: 0.412717, Accuracy: 85.71%\n",
      "Cost after iteration 2000: 0.238421, Accuracy: 100.00%\n",
      "Cost after iteration 3000: 0.162656, Accuracy: 100.00%\n",
      "Cost after iteration 4000: 0.119458, Accuracy: 100.00%\n",
      "Cost after iteration 5000: 0.094788, Accuracy: 100.00%\n",
      "Cost after iteration 6000: 0.076591, Accuracy: 100.00%\n",
      "Cost after iteration 7000: 0.062894, Accuracy: 100.00%\n",
      "Cost after iteration 8000: 0.052424, Accuracy: 100.00%\n",
      "Cost after iteration 9000: 0.044308, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Updated sample data\n",
    "X = np.array([[0, 1, 4, 5, 2, 5, 3], [2, 3, 5, 6, 2, 7, 4]]) # Input features ,shape=(nx, m), m:training examples, nx: #features\n",
    "Y = np.array([[0, 0, 1, 1, 0, 1, 0]])                   # Output labels\n",
    "\n",
    "parameters = nn_model(X, Y, 5, num_iterations = 10000, print_cost=True, learning_rate = 0.01 )\n",
    "\n",
    "W1 = parameters[\"W1\"]\n",
    "b1 = parameters[\"b1\"]\n",
    "W2 = parameters[\"W2\"]\n",
    "b2 = parameters[\"b2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "34332c6e-a991-4c44-925e-5734e7d267c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1.18521062e+00, -1.56561054e-01],\n",
       "        [-1.93231620e-02, -2.27658143e-03],\n",
       "        [-1.25176114e-02, -7.81241153e-04],\n",
       "        [ 8.98739180e-01, -1.22583608e-01],\n",
       "        [ 5.87120447e-03, -5.85625696e-03]]),\n",
       " array([[-2.45028576e+00],\n",
       "        [ 0.00000000e+00],\n",
       "        [ 0.00000000e+00],\n",
       "        [-1.84992332e+00],\n",
       "        [-3.45574402e-05]]),\n",
       " array([[ 2.72632243e+00,  1.36743310e-02, -2.87543899e-03,\n",
       "          2.06031188e+00,  2.31045407e-03]]),\n",
       " array([[-4.07940946]]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e6960525-f43e-4ae2-941e-17df7181d4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions:\n",
      "[[0. 0. 1. 1. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Use the trained weights and biases to make predictions\n",
    "predictions = predict(W1, b1, W2, b2, X)\n",
    "\n",
    "print(\"Predictions:\")\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f698882-3f59-4275-ace4-b35fdedf86be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
